<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>L.1. Setting up the multi-arch Linux LXC container farm for NUT CI</title><link rel="stylesheet" type="text/css" href="docbook-xsl.css" /><meta name="generator" content="DocBook XSL Stylesheets Vsnapshot" /><link rel="home" href="index.html" title="Network UPS Tools User Manual" /><link rel="up" href="CI_Farm_Notes.html" title="L. CI Farm configuration notes" /><link rel="prev" href="CI_Farm_Notes.html" title="L. CI Farm configuration notes" /><link rel="next" href="_connecting_jenkins_to_the_containers.html" title="L.2. Connecting Jenkins to the containers" /><meta xmlns="" name="format-detection" content="telephone=no" /></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><td width="20%" align="left"><a accesskey="p" href="CI_Farm_Notes.html">Prev</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="_connecting_jenkins_to_the_containers.html">Next</a></td></tr></table><hr /></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_setting_up_the_multi_arch_linux_lxc_container_farm_for_nut_ci"></a>L.1. Setting up the multi-arch Linux LXC container farm for NUT CI</h3></div></div></div><p>Due to some historical reasons including earlier personal experience,
the Linux container setup implemented as described below was done with
persistent LXC containers wrapped by LIBVIRT for management. There was
no particular use-case for systems like Docker (and no firepower for a
Kubernetes cluster) in that the build environment intended for testing
non-regression against a certain release does not need to be regularly
updated — its purpose is to be stale and represent what users still
running that system for whatever reason (e.g. embedded, IoT, corporate)
have in their environments.</p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_common_preparations"></a>Common preparations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
Prepare LXC and LIBVIRT-LXC integration, including an "independent"
  (aka "masqueraded) bridge for NAT, following <a class="ulink" href="https://wiki.debian.org/LXC" target="_top">https://wiki.debian.org/LXC</a>
  and <a class="ulink" href="https://wiki.debian.org/LXC/SimpleBridge" target="_top">https://wiki.debian.org/LXC/SimpleBridge</a>
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p class="simpara">
For dnsmasq integration on the independent bridge (<code class="literal">lxcbr0</code> following
   the documentation examples), be sure to mention:
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: square; "><li class="listitem">
<code class="literal">LXC_DHCP_CONFILE="/etc/lxc/dnsmasq.conf"</code> in <code class="literal">/etc/default/lxc-net</code>
</li><li class="listitem">
<code class="literal">dhcp-hostsfile=/etc/lxc/dnsmasq-hosts.conf</code> in/as the content of
    <code class="literal">/etc/lxc/dnsmasq.conf</code>
</li><li class="listitem">
<code class="literal">touch /etc/lxc/dnsmasq-hosts.conf</code> which would list simple <code class="literal">name,IP</code>
    pairs, one per line (so one per container)
</li><li class="listitem">
<code class="literal">systemctl restart lxc-net</code> to apply config (is this needed after
    setup of containers too, to apply new items before booting them?)
</li></ul></div></li></ul></div></li><li class="listitem">
Install qemu with its <code class="literal">/usr/bin/qemu-*-static</code> and registration in
  <code class="literal">/var/lib/binfmt</code>
</li><li class="listitem">
Prepare an LVM partition (or preferably some other tech like ZFS)
  as <code class="literal">/srv/libvirt</code> and create a <code class="literal">/srv/libvirt/rootfs</code> to hold the containers
</li><li class="listitem"><p class="simpara">
Prepare <code class="literal">/home/abuild</code> on the host system (preferably in ZFS with
  lightweight compression like lz4 — and optionally, only if the amount
  of available system RAM permits, with deduplication; otherwise avoid it);
  account user and group ID numbers are <code class="literal">399</code> as on the rest of the CI farm
  (historically, inherited from OBS workers)
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p class="simpara">
It may help to generate an ssh key without a passphrase for <code class="literal">abuild</code>
   that it would trust, to sub-login from CI agent sessions into the
   container. Then again, it may be not required if CI logs into the
   host by SSH using <code class="literal">authorized_keys</code> and an SSH Agent, and the inner
   ssh client would forward that auth channel to the original agent.
</p><pre class="screen">abuild$ ssh-keygen
# accept defaults

abuild$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
abuild$ chmod 640 ~/.ssh/authorized_keys</pre></li></ul></div></li><li class="listitem"><p class="simpara">
Edit the root (or whoever manages libvirt) <code class="literal">~/.profile</code> to default the
  virsh provider with:
</p><pre class="screen">LIBVIRT_DEFAULT_URI=lxc:///system
export LIBVIRT_DEFAULT_URI</pre></li><li class="listitem"><p class="simpara">
If host root filesystem is small, relocate the LXC download cache to the
  (larger) <code class="literal">/srv/libvirt</code> partition:
</p><pre class="screen">:; mkdir -p /srv/libvirt/cache-lxc
:; rm -rf /var/cache/lxc
:; ln -sfr /srv/libvirt/cache-lxc /var/cache/lxc</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
Maybe similarly relocate shared <code class="literal">/home/abuild</code> to reduce strain on rootfs?
</li></ul></div></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_setup_a_container"></a>Setup a container</h4></div></div></div><p>Note that completeness of qemu CPU emulation varies, so not all distros
can be installed, e.g. "s390x" failed for both debian10 and debian11 to
set up the <code class="literal">openssh-server</code> package, or once even to run <code class="literal">/bin/true</code> (seems
to have installed an older release though, to match the outdated emulation?)</p><p>While the <code class="literal">lxc-create</code> tool does not really specify the error cause and
deletes the directories after failure, it shows the pathname where it
writes the log (also deleted). Before re-trying the container creation, this
file can be watched with e.g. <code class="literal">tail -F /var/cache/lxc/.../debootstrap.log</code></p><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>You can find the list of LXC "template" definitions on your system
by looking at the contents of the <code class="literal">/usr/share/lxc/templates/</code> directory,
e.g. a script named <code class="literal">lxc-debian</code> for the "debian" template. You can see
further options for each "template" by invoking its help action, e.g.:</p><pre class="screen">:; lxc-create -t debian -h</pre></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="_initial_container_installation_for_various_guest_oses"></a>Initial container installation (for various guest OSes)</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
Install containers like this:
</p><pre class="screen">:; lxc-create -P /srv/libvirt/rootfs \
    -n jenkins-debian11-mips64el -t debian -- \
    -r bullseye -a mips64el</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p class="simpara">
to specify a particular mirror (not everyone hosts everything —    so if you get something like
   <code class="literal">"E: Invalid Release file, no entry for main/binary-mips/Packages"</code>
   then see <a class="ulink" href="https://www.debian.org/mirror/list" target="_top">https://www.debian.org/mirror/list</a> for details, and double-check
   the chosen site to verify if the distro version of choice is hosted with
   your arch of choice):
</p><pre class="screen">:; MIRROR="http://ftp.br.debian.org/debian/" \
   lxc-create -P /srv/libvirt/rootfs \
    -n jenkins-debian10-mips -t debian -- \
    -r buster -a mips</pre></li><li class="listitem"><p class="simpara">
…or for EOLed distros, use the Debian Archive server.
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: square; "><li class="listitem"><p class="simpara">
Install the container with Debian Archive as the mirror like this:
</p><pre class="screen">:; MIRROR="http://archive.debian.org/debian-archive/debian/" \
   lxc-create -P /srv/libvirt/rootfs \
    -n jenkins-debian8-s390x -t debian -- \
    -r jessie -a s390x</pre></li><li class="listitem"><p class="simpara">
Note you may have to add trust to their (now expired) GPG keys for
    packaging to verify signatures made at the time the key was valid,
    by un-symlinking (if appropriate) the debootstrap script such as
    <code class="literal">/usr/share/debootstrap/scripts/jessie</code>, commenting away the
    <code class="literal">keyring /usr/share/keyrings/debian-archive-keyring.gpg</code> line and
    setting <code class="literal">keyring /usr/share/keyrings/debian-archive-removed-keys.gpg</code>
    instead. You may further have to edit <code class="literal">/usr/share/debootstrap/functions</code>
    and/or <code class="literal">/usr/share/lxc/templates/lxc-debian</code> to honor that setting (the
    <code class="literal">releasekeyring</code> was hard-coded in version I had installed), e.g. in the
    latter file ensure such logic as below, and re-run the installation:
</p><pre class="screen">...
    # If debian-archive-keyring isn't installed, fetch GPG keys directly
    releasekeyring="`grep -E '^keyring ' "/usr/share/debootstrap/scripts/$release" | sed -e 's,^keyring ,,' -e 's,[ #].*$,,'`" 2&gt;/dev/null
    if [ -z $releasekeyring ]; then
        releasekeyring=/usr/share/keyrings/debian-archive-keyring.gpg
    fi
    if [ ! -f $releasekeyring ]; then
...</pre></li></ul></div></li><li class="listitem"><p class="simpara">
…Alternatively, other distributions can be used (as supported by your
   LXC scripts, typically in <code class="literal">/usr/share/debootstrap/scripts</code>), e.g. Ubuntu:
</p><pre class="screen">:; lxc-create -P /srv/libvirt/rootfs \
    -n jenkins-ubuntu1804-s390x -t ubuntu -- \
    -r bionic -a s390x</pre></li><li class="listitem"><p class="simpara">
For distributions with a different packaging mechanism from that on the
   LXC host system, you may need to install corresponding tools (e.g. <code class="literal">yum4</code>,
   <code class="literal">rpm</code> and <code class="literal">dnf</code> on Debian hosts for installing CentOS and related guests).
   You may also need to pepper with symlinks to taste (e.g. <code class="literal">yum =&gt; yum4</code>),
   or find a <code class="literal">pacman</code> build to install Arch Linux or derivative, etc.
   Otherwise, you risk seeing something like this:
</p><pre class="screen">root@debian:~# lxc-create -P /srv/libvirt/rootfs \
    -n jenkins-centos7-x86-64 -t centos -- \
    -R 7 -a x86_64

Host CPE ID from /etc/os-release:
'yum' command is missing
lxc-create: jenkins-centos7-x86-64: lxccontainer.c:
  create_run_template: 1616 Failed to create container from template
lxc-create: jenkins-centos7-x86-64: tools/lxc_create.c:
  main: 319 Failed to create container jenkins-centos7-x86-64</pre><p class="simpara">Note also that with such "third-party" distributions you may face other
   issues; for example, the CentOS helper did not generate some fields in
   the <code class="literal">config</code> file that were needed for conversion into libvirt "domxml"
   (as found by trial and error, and comparison to other <code class="literal">config</code> files):</p><pre class="screen">lxc.uts.name = jenkins-centos7-x86-64
lxc.arch = x86_64</pre><p class="simpara">Also note the container/system naming without underscore in "x86_64" —    the deployed system discards the character when assigning its hostname.
   Using "amd64" is another reasonable choice here.
** For Arch Linux you would need <code class="literal">pacman</code> tools on the host system, so see
   <a class="ulink" href="https://wiki.archlinux.org/title/Install_Arch_Linux_from_existing_Linux#Using_pacman_from_the_host_system" target="_top">https://wiki.archlinux.org/title/Install_Arch_Linux_from_existing_Linux#Using_pacman_from_the_host_system</a>
   for details.
   On a Debian/Ubuntu host (assumed ready for NUT builds per
   linkdoc:config-prereqs.txt) it would start like this:</p><pre class="screen">:; apt-get update
:; apt-get install meson ninja-build cmake

# Some dependencies for pacman itself; note there are several libcurl builds;
# pick another if your system constraints require you to:
:; apt-get install libarchive-dev libcurl4-nss-dev gpg libgpgme-dev

:; git clone https://gitlab.archlinux.org/pacman/pacman.git
:; cd pacman

# Iterate something like this until all needed dependencies fall
# into line (note libdir for your host architecture):
:; rm -rf build; mkdir build &amp;&amp; meson build --libdir=/usr/lib/x86_64-linux-gnu

:; ninja -C build
# Depending on your asciidoc version, it may require that `--asciidoc-opts` are
# passed as equation to a vale (not space-separated from it). Then apply this:
#   diff --git a/doc/meson.build b/doc/meson.build
#   -      '--asciidoc-opts', ' '.join(asciidoc_opts),
#   +      '--asciidoc-opts='+' '.join(asciidoc_opts),
# and re-run (meson and) ninja.

# Finally when all succeeded:
:; sudo ninja -C build install
:; cd</pre><p class="simpara">You will also need <code class="literal">pacstrap</code> and Debian <code class="literal">arch-install-scripts</code> package
does not deliver it. It is however simply achieved:</p></li></ul></div></li></ul></div><pre class="screen">:; git clone https://github.com/archlinux/arch-install-scripts
:; cd arch-install-scripts
:; make &amp;&amp; sudo make PREFIX=/usr install
:; cd</pre><p>+
It will also want an <code class="literal">/etc/pacman.d/mirrorlist</code> which you can populate for
your geographic location from <a class="ulink" href="https://archlinux.org/mirrorlist/" target="_top">https://archlinux.org/mirrorlist/</a> service,
or just fetch them all (don’t forget to uncomment some <code class="literal">Server =</code> lines):</p><pre class="screen">:; mkdir -p /etc/pacman.d/
:; curl https://archlinux.org/mirrorlist/all/ &gt; /etc/pacman.d/mirrorlist</pre><p>+
And to reference it from your host <code class="literal">/etc/pacman.conf</code> by un-commenting the
<code class="literal">[core]</code> section and <code class="literal">Include</code> instruction, as well as adding <code class="literal">[community]</code>
and <code class="literal">[extra]</code> sections with same reference, e.g.:</p><pre class="screen">[core]
### SigLevel = Never
SigLevel = PackageRequired
Include = /etc/pacman.d/mirrorlist

[extra]
### SigLevel = Never
SigLevel = PackageRequired
Include = /etc/pacman.d/mirrorlist

[community]
### SigLevel = Never
SigLevel = PackageRequired
Include = /etc/pacman.d/mirrorlist</pre><p>+
And just then you can proceed with LXC:</p><pre class="screen">:; lxc-create -P /srv/libvirt/rootfs \
    -n jenkins-archlinux-amd64 -t archlinux -- \
    -a x86_64 -P openssh,sudo</pre><p>+
In my case, it had problems with GPG keyring missing (using one in host
system, as well as the package cache outside the container, it seems)
so I had to run <code class="literal">pacman-key --init; pacman-key --refresh-keys</code> on the
host itself. Even so, <code class="literal">lxc-create</code> complained about updating some keyring
entries and I had to go one by one picking key servers (serving different
metadata) like this:</p><pre class="screen">:; pacman-key --keyserver keyserver.ubuntu.com --recv-key 6D1655C14CE1C13E</pre><p>+
In the worst case, see <code class="literal">SigLevel = Never</code> for <code class="literal">pacman.conf</code> to not check
package integrity (seems too tied into thinking that host OS is Arch)…</p><p>+
It seems that pre-fetching the package databases with <code class="literal">pacman -Sy</code> on
the host was also important.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="_initial_container_related_setup"></a>Initial container-related setup</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
Add the "name,IP" line for this container to <code class="literal">/etc/lxc/dnsmasq-hosts.conf</code>
  on the host, e.g.:
</p><pre class="screen">jenkins-debian11-mips,10.0.3.245</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>Don’t forget to eventually <code class="literal">systemctl restart lxc-net</code> to apply the
new host reservation!</p></div></li><li class="listitem"><p class="simpara">
Convert a pure LXC container to be managed by LIBVIRT-LXC (and edit config
  markup on the fly — e.g. fix the LXC <code class="literal">dir:/</code> URL schema):
</p><pre class="screen">:; virsh -c lxc:///system domxml-from-native lxc-tools \
    /srv/libvirt/rootfs/jenkins-debian11-armhf/config \
    | sed -e 's,dir:/srv,/srv,' \
    &gt; /tmp/x &amp;&amp; virsh define /tmp/x</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>You may want to tune the default generic 64MB RAM allocation,
   so your launched QEMU containers are not OOM-killed as they exceeded
   their memory <code class="literal">cgroup</code> limit. In practice they do not eat <span class="strong"><strong>that much</strong></span>
   resident memory, just want to have it addressable by VMM, I guess
   (swap is not very used either), at least not until active builds
   start (and then it depends on compiler appetite and <code class="literal">make</code> program
   parallelism level you allow, e.g. by pre-exporting <code class="literal">MAXPARMAKES</code>
   environment variable for <code class="literal">ci_build.sh</code>, and on the number of Jenkins
   "executors" assigned to the build agent).</p></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p class="simpara">
It may be needed to revert the generated "os/arch" to <code class="literal">x86_64</code> (and let
   QEMU handle the rest) in the <code class="literal">/tmp/x</code> file, and re-try the definition:
</p><pre class="screen">:; virsh define /tmp/x</pre></li></ul></div></li><li class="listitem"><p class="simpara">
Then execute <code class="literal">virsh edit jenkins-debian11-armhf</code> (and same for other
  containers) to bind-mount the common <code class="literal">/home/abuild</code> location, adding
  this tag to their "devices":
</p><pre class="screen">    &lt;filesystem type='mount' accessmode='passthrough'&gt;
      &lt;source dir='/home/abuild'/&gt;
      &lt;target dir='/home/abuild'/&gt;
    &lt;/filesystem&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
Note that generated XML might not conform to current LXC schema, so it
   fails validation during save; this can be bypassed with <code class="literal">i</code> when it asks.
   One such case was however with indeed invalid contents, the "dir:" schema
   removed by example above.
</li></ul></div></li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_shepherd_the_herd"></a>Shepherd the herd</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
Monitor deployed container rootfs’es with:
</p><pre class="screen">:; du -ks /srv/libvirt/rootfs/*</pre><p class="simpara">(should have non-trivial size for deployments without fatal infant errors)</p></li><li class="listitem"><p class="simpara">
Mass-edit/review libvirt configurations with:
</p><pre class="screen">:; virsh list --all | awk '{print $2}' \
   | grep jenkins | while read X ; do \
     virsh edit --skip-validate $X ; done</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
…or avoid <code class="literal">--skip-validate</code> when markup is initially good :)
</li></ul></div></li><li class="listitem"><p class="simpara">
Mass-define network interfaces:
</p><pre class="screen">:; virsh list --all | awk '{print $2}' \
   | grep jenkins | while read X ; do \
     virsh dumpxml "$X" | grep "bridge='lxcbr0'" \
     || virsh attach-interface --domain "$X" --config \
        --type bridge --source lxcbr0 ; \
   done</pre></li><li class="listitem"><p class="simpara">
Verify that unique MAC addresses were defined (e.g. <code class="literal">00:16:3e:00:00:01</code>
  tends to pop up often, while <code class="literal">52:54:00:xx:xx:xx</code> are assigned to other
  containers); edit the domain definitions to randomize, if needed:
</p><pre class="screen">:; grep 'mac add' /etc/libvirt/lxc/*.xml | awk '{print $NF" "$1}' | sort</pre></li><li class="listitem"><p class="simpara">
Make sure at least one console device exists (end of file, under the
  network interface definition tags), e.g.:
</p><pre class="screen">    &lt;console type='pty'&gt;
      &lt;target type='lxc' port='0'/&gt;
    &lt;/console&gt;</pre></li><li class="listitem"><p class="simpara">
Populate with <code class="literal">abuild</code> account, as well as with the <code class="literal">bash</code> shell and
  <code class="literal">sudo</code> ability, reporting of assigned IP addresses on the console,
  and SSH server access complete with envvar passing from CI clients
  by virtue of <code class="literal">ssh -o SendEnv='*' container-name</code>:
</p><pre class="screen">:; for ALTROOT in /srv/libvirt/rootfs/*/rootfs/ ; do \
    echo "=== $ALTROOT :" &gt;&amp;2; \
    grep eth0 "$ALTROOT/etc/issue" || ( printf '%s %s\n' \
        '\S{NAME} \S{VERSION_ID} \n \l@\b ;' \
        'Current IP(s): \4{eth0} \4{eth1} \4{eth2} \4{eth3}' \
        &gt;&gt; "$ALTROOT/etc/issue" ) ; \
    grep eth0 "$ALTROOT/etc/issue.net" || ( printf '%s %s\n' \
        '\S{NAME} \S{VERSION_ID} \n \l@\b ;' \
        'Current IP(s): \4{eth0} \4{eth1} \4{eth2} \4{eth3}' \
        &gt;&gt; "$ALTROOT/etc/issue.net" ) ; \
    groupadd -R "$ALTROOT" -g 399 abuild ; \
    useradd -R "$ALTROOT" -u 399 -g abuild -M -N -s /bin/bash abuild \
    || useradd -R "$ALTROOT" -u 399 -g 399 -M -N -s /bin/bash abuild \
    || { if ! grep -w abuild "$ALTROOT/etc/passwd" ; then \
            echo 'abuild:x:399:399::/home/abuild:/bin/bash' \
            &gt;&gt; "$ALTROOT/etc/passwd" ; \
            echo "USERADDed manually: passwd" &gt;&amp;2 ; \
         fi ; \
         if ! grep -w abuild "$ALTROOT/etc/shadow" ; then \
            echo 'abuild:!:18889:0:99999:7:::' &gt;&gt; "$ALTROOT/etc/shadow" ; \
            echo "USERADDed manually: shadow" &gt;&amp;2 ; \
         fi ; \
       } ; \
    if [ -s "$ALTROOT/etc/ssh/sshd_config" ]; then \
        grep 'AcceptEnv \*' "$ALTROOT/etc/ssh/sshd_config" || ( \
            ( echo "" ; \
              echo "# For CI: Allow passing any envvars:"; \
              echo 'AcceptEnv *' ) \
            &gt;&gt; "$ALTROOT/etc/ssh/sshd_config" \
        ) ; \
    fi ; \
   done</pre><p class="simpara">Note that for some reason, in some of those other-arch distros <code class="literal">useradd</code>
fails to find the group anyway; then we have to "manually" add them.</p></li><li class="listitem"><p class="simpara">
Let the host know and resolve the names/IPs of containers you assigned:
</p><pre class="screen">:; grep -v '#' /etc/lxc/dnsmasq-hosts.conf \
   | while IFS=, read N I ; do \
    getent hosts "$N" &gt;&amp;2 || echo "$I $N" ; \
   done &gt;&gt; /etc/hosts</pre></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_further_setup_of_the_containers"></a>Further setup of the containers</h4></div></div></div><p>See NUT <code class="literal">docs/config-prereqs.txt</code> about dependency package installation
for Debian-based Linux systems.</p><p>It may be wise to not install e.g. documentation generation tools (or at
least not the full set for HTML/PDF generation) in each environment, in
order to conserve space and run-time stress.</p><p>Still, if there are significant version outliers (such as using an older
distribution due to vCPU requirements), it can be installed fully just
to ensure non-regression — e.g. that when adapting <code class="literal">Makefile</code> rule
definitions or compiler arguments to modern toolkits, we do not lose
the ability to build with older ones.</p><p>For this, <code class="literal">chroot</code> from the host system can be used, e.g. to improve the
interactive usability for a population of Debian(-compatible) containers
(and to use its networking, while the operating environment in containers
may be not yet configured or still struggling to access the Internet):</p><pre class="screen">:; for ALTROOT in /srv/libvirt/rootfs/*/rootfs/ ; do \
    echo "=== $ALTROOT :" ; \
    chroot "$ALTROOT" apt-get install \
        sudo bash vim mc p7zip p7zip-full pigz pbzip2 git \
   ; done</pre><p>Similarly for <code class="literal">yum</code>-managed systems (CentOS and relatives), though specific
package names can differ, and additional package repositories may need to
be enabled first (see <a class="ulink" href="config-prereqs.txt" target="_top">config-prereqs.txt</a> for more
details such as recommended package names).</p><p>Note that technically <code class="literal">(sudo) chroot ...</code> can also be used from the CI worker
account on the host system to build in the prepared filesystems without the
overhead of running containers as complete operating environments with any
standard services and several copies of Jenkins <code class="literal">agent.jar</code> in them.</p><p>Also note that externally-driven set-up of some packages, including the
<code class="literal">ca-certificates</code> and the JDK/JRE, require that the <code class="literal">/proc</code> filesystem
is usable in the chroot environment. This can be achieved with e.g.:</p><pre class="screen">:; for ALTROOT in /srv/libvirt/rootfs/*/rootfs/ ; do \
    for D in proc ; do \
      echo "=== $ALTROOT/$D :" ; \
      mkdir -p "$ALTROOT/$D" ; \
      mount -o bind,rw "/$D" "$ALTROOT/$D" ; \
    done ; \
   done</pre><p>TODO: Test and document a working NAT and firewall setup for this, to allow
SSH access to the containers via dedicated TCP ports exposed on the host.</p><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="_arch_linux_containers"></a>Arch Linux containers</h5></div></div></div><p>Arch Linux containers prepared by procedure above include only a minimal
footprint, and if you missed the <code class="literal">-P pkg,list</code> argument, they can lack
even an SSH server. Suggestions below assume this path to container:</p><pre class="screen">:; ALTROOT=/srv/libvirt/rootfs/jenkins-archlinux-amd64/rootfs/</pre><p>Let <code class="literal">pacman</code> know current package database:</p><pre class="screen">:; grep 8.8.8.8 $ALTROOT/etc/resolv.conf || (echo 'nameserver 8.8.8.8' &gt; $ALTROOT/etc/resolv.conf)
:; chroot $ALTROOT pacman -Syu
:; chroot $ALTROOT pacman -S openssh sudo
:; chroot $ALTROOT systemctl enable sshd
:; chroot $ALTROOT systemctl start sshd</pre><p>This may require that you perform bind-mounts above, as well as "passthrough"
the <code class="literal">/var/cache/pacman/pkg</code> from host to guest environment (in <code class="literal">virsh edit</code>,
and bind-mount for <code class="literal">chroot</code> like for <code class="literal">/proc</code> et al above).</p><p>It is possible that <code class="literal">virsh console</code> would serve you better than <code class="literal">chroot</code>.
Note you may have to first <code class="literal">chroot</code> to set the <code class="literal">root</code> password anyhow.</p></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_troubleshooting"></a>Troubleshooting</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
Q: Container won’t start, its <code class="literal">virsh console</code> says something like:
</p><pre class="screen">Failed to create symlink /sys/fs/cgroup/net_cls: Operation not permitted</pre></li></ul></div><p>A: According to <a class="ulink" href="https://bugzilla.redhat.com/show_bug.cgi?id=1770763" target="_top">https://bugzilla.redhat.com/show_bug.cgi?id=1770763</a>
   (skip to the end for summary) this can happen when a newer Linux
   host system with <code class="literal">cgroupsv2</code> capabilities runs an older guest distro
   which only knows about <code class="literal">cgroupsv1</code>, such as when hosting a CentOS 7
   container on a Debian 11 server.
** One workaround is to ensure that the guest <code class="literal">systemd</code> does not try to
   "join" host facilities, by setting an explicit empty list for that:</p><p>+</p><pre class="screen">:; echo 'JoinControllers=' &gt;&gt; "$ALTROOT/etc/systemd/system.conf"</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
Another approach is to upgrade <code class="literal">systemd</code> related packages in the guest
   container. This may require additional "backport" repositories or
   similar means, possibly maintained not by distribution itself but by
   other community members, and arguably would logically compromise the
   idea of non-regression builds in the old environment "as is".
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p class="simpara">
Q: Server was set up with ZFS as recommended, and lots of I/O hit the
  disk even when application writes are negligible
</p><p class="simpara">A: This was seen on some servers and generally derives from data layout
   and how ZFS maintains the tree structure of blocks. A small application
   write (such as a new log line) means a new empty data block allocation,
   an old block release, and bubble up through the whole metadata tree to
   complete the transaction (grouped as TXG to flush to disk).</p></li></ul></div></li><li class="listitem">
One solution is to use discardable build workspaces in RAM-backed
   storage like <code class="literal">/dev/shm</code> (<code class="literal">tmpfs</code>) on Linux, or <code class="literal">/tmp</code> (<code class="literal">swap</code>) on
   illumos hosting systems, and only use persistent storage for the home
   directory with <code class="literal">.ccache</code> and <code class="literal">.gitcache-dynamatrix</code> directories.
</li><li class="listitem">
Another solution is to reduce the frequency of TXG sync from modern
   default of 5 sec to conservative 30-60 sec. Check how to set the
   <code class="literal">zfs_txg_timeout</code> on your platform.
</li></ul></div></div></div><div xmlns="" class="navfooter nut_footer"><hr />
		Last updated 2023-11-03 02:44:56 -- Network UPS Tools 2.8.1.1</div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="CI_Farm_Notes.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="CI_Farm_Notes.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="_connecting_jenkins_to_the_containers.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top"> </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> </td></tr></table></div></body></html>