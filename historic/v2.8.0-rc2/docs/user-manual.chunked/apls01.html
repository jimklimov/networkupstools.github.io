<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>L.1. Setting up the multi-arch Linux LXC container farm for NUT CI</title><link rel="stylesheet" type="text/css" href="docbook-xsl.css" /><meta name="generator" content="DocBook XSL Stylesheets Vsnapshot" /><link rel="home" href="index.html" title="Network UPS Tools User Manual" /><link rel="up" href="apl.html" title="L. CI Farm configuration notes" /><link rel="prev" href="apl.html" title="L. CI Farm configuration notes" /><meta xmlns="" name="format-detection" content="telephone=no" /></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><td width="20%" align="left"><a accesskey="p" href="apl.html">Prev</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> </td></tr></table><hr /></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_setting_up_the_multi_arch_linux_lxc_container_farm_for_nut_ci"></a>L.1. Setting up the multi-arch Linux LXC container farm for NUT CI</h3></div></div></div><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Two NUT websites</h3><p>This version of the page reflects NUT release <span class="emphasis"><em>v2.8.0-rc2</em></span>
with codebase commited f28ec588e at 2022-04-10T12:11:35+00:00</p><p>Options, features and capabilities in current development (and future
releases) are detailed on the main site and may differ from ones
described here.</p></div><p>Due to some historical reasons including earlier personal experience,
the Linux container setup implemented as described below was done with
persistent LXC containers wrapped by LIBVIRT for management. There was
no particular use-case for systems like Docker (and no firepower for a
Kubernetes cluster) in that the build environment intended for testing
non-regression against a certain release does not need to be regularly
updated — its purpose is to be stale and represent what users still
running that system for whatever reason (e.g. embedded, IoT, corporate)
have in their environments.</p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_common_preparations"></a>Common preparations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
Prepare LXC and LIBVIRT-LXC integration, including an "independent"
  (aka "masqueraded) bridge for NAT, following <a class="ulink" href="https://wiki.debian.org/LXC" target="_top">https://wiki.debian.org/LXC</a>
  and <a class="ulink" href="https://wiki.debian.org/LXC/SimpleBridge" target="_top">https://wiki.debian.org/LXC/SimpleBridge</a>
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p class="simpara">
For dnsmasq integration on the independent bridge (<code class="literal">lxcbr0</code> following
   the documentation examples), be sure to mention:
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: square; "><li class="listitem">
<code class="literal">LXC_DHCP_CONFILE="/etc/lxc/dnsmasq.conf"</code> in <code class="literal">/etc/default/lxc-net</code>
</li><li class="listitem">
<code class="literal">dhcp-hostsfile=/etc/lxc/dnsmasq-hosts.conf</code> in/as the content of
    <code class="literal">/etc/lxc/dnsmasq.conf</code>
</li><li class="listitem">
<code class="literal">touch /etc/lxc/dnsmasq-hosts.conf</code> which would list simple <code class="literal">name,IP</code>
    pairs, one per line (so one per container)
</li><li class="listitem">
<code class="literal">systemctl restart lxc-net</code> to apply config (is this needed after
    setup of containers too, to apply new items before booting them?)
</li></ul></div></li></ul></div></li><li class="listitem">
Install qemu with its <code class="literal">/usr/bin/qemu-*-static</code> and registration in
  <code class="literal">/var/lib/binfmt</code>
</li><li class="listitem">
Prepare an LVM partition (or preferably some other tech like ZFS)
  as <code class="literal">/srv/libvirt</code> and create a <code class="literal">/srv/libvirt/rootfs</code> to hold the containers
</li><li class="listitem"><p class="simpara">
Prepare <code class="literal">/home/abuild</code> on the host system (preferably in ZFS with dedup);
  account user and group ID numbers are <code class="literal">399</code> as on the rest of the CI farm
  (historically, inherited from OBS workers)
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p class="simpara">
It may help to generate an ssh key without a passphrase for <code class="literal">abuild</code>
   that it would trust, to sub-login from CI agent sessions into the
   container. Then again, it may be not required if CI logs into the
   host by SSH using authorized_keys and an SSH Agent, and the inner
   ssh client would forward that auth channel to the original agent.
</p><pre class="screen">abuild$ ssh-keygen
# accept defaults

abuild$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
abuild$ chmod 640 ~/.ssh/authorized_keys</pre></li></ul></div></li><li class="listitem"><p class="simpara">
Edit the root (or whoever manages libvirt) <code class="literal">~/.profile</code> to default the
  virsh provider with:
</p><pre class="screen">LIBVIRT_DEFAULT_URI=lxc:///system
export LIBVIRT_DEFAULT_URI</pre></li><li class="listitem"><p class="simpara">
If host root filesystem is small, relocate the LXC download cache to the
  (larger) <code class="literal">/srv/libvirt</code> partition:
</p><pre class="screen">:; mkdir -p /srv/libvirt/cache-lxc
:; rm -rf /var/cache/lxc
:; ln -sfr /srv/libvirt/cache-lxc /var/cache/lxc</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
Maybe similarly relocate shared <code class="literal">/home/abuild</code> to reduce strain on rootfs?
</li></ul></div></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_setup_a_container"></a>Setup a container</h4></div></div></div><p>Note that completeness of qemu CPU emulation varies, so not all distros
can be installed, e.g. "s390x" failed for both debian10 and debian11 to
set up the openssh-server package, or once even to run /bin/true (seems
to have installed an older release though, to match the outdated emulation?)</p><p>While the <code class="literal">lxc-create</code> tool does not really specify the error cause and
deletes the directories after failure, it shows the pathname where it
writes the log (also deleted). Before re-trying the container creation, this
file can be watched with e.g. <code class="literal">tail -F /var/cache/lxc/.../debootstrap.log</code></p><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>You can find the list of LXC "template" definitions on your system
by looking at the contents of the <code class="literal">/usr/share/lxc/templates/</code> directory,
e.g. a script named <code class="literal">lxc-debian</code> for the "debian" template. You can see
further options for each "template" by invoking its help action, e.g.:</p></div><p>+</p><pre class="screen">:; lxc-create -t debian -h</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
Install containers like this:
</p><pre class="screen">:; lxc-create -n jenkins-debian11-mips64el -P /srv/libvirt/rootfs -t debian -- \
    -r bullseye -a mips64el</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p class="simpara">
to specify a particular mirror (not everyone hosts everything —    so if you get something like
   <code class="literal">"E: Invalid Release file, no entry for main/binary-mips/Packages"</code>
   then see <a class="ulink" href="https://www.debian.org/mirror/list" target="_top">https://www.debian.org/mirror/list</a> for details, and double-check
   the chosen site to verify if the distro version of choice is hosted with
   your arch of choice):
</p><pre class="screen">:; MIRROR="http://ftp.br.debian.org/debian/" \
   lxc-create -n jenkins-debian10-mips -P /srv/libvirt/rootfs -t debian -- \
    -r buster -a mips</pre></li><li class="listitem"><p class="simpara">
…or for EOLed distros, use the archive, e.g.:
</p><pre class="screen">:; MIRROR="http://archive.debian.org/debian-archive/debian/" \
   lxc-create -n jenkins-debian8-s390x -P /srv/libvirt/rootfs -t debian -- \
    -r jessie -a s390x</pre></li><li class="listitem"><p class="simpara">
…Alternatively, other distributions can be used (as supported by your
   LXC scripts, typically in <code class="literal">/usr/share/debootstrap/scripts</code>), e.g. Ubuntu:
</p><pre class="screen">:; lxc-create -n jenkins-ubuntu1804-s390x -P /srv/libvirt/rootfs -t ubuntu -- \
    -r bionic -a s390x</pre></li><li class="listitem"><p class="simpara">
For distributions with a different packaging mechanism from that on the
   LXC host system, you may need to install corresponding tools (e.g. <code class="literal">yum4</code>,
   <code class="literal">rpm</code> and <code class="literal">dnf</code> on Debian hosts for installing CentOS and related guests).
   You may also need to pepper with symlinks to taste (e.g. <code class="literal">yum =&gt; yum4</code>),
   or find a <code class="literal">pacman</code> build to install Arch Linux or derivative, etc.
   Otherwise, you risk seeing something like this:
</p><pre class="screen">root@debian:~# lxc-create -n jenkins-centos7-x86-64 -P /srv/libvirt/rootfs \
    -t centos -- -R 7 -a x86_64
Host CPE ID from /etc/os-release:
'yum' command is missing
lxc-create: jenkins-centos7-x86-64: lxccontainer.c: create_run_template: 1616 Failed to create container from template
lxc-create: jenkins-centos7-x86-64: tools/lxc_create.c: main: 319 Failed to create container jenkins-centos7-x86-64</pre><pre class="literallayout">Note also that with such "third-party" distributions you may face other
issues; for example, the CentOS helper did not generate some fields in
the `config` file that were needed for conversion into libvirt "domxml"
(as found by trial and error, and comparison to other `config` files):</pre><pre class="screen">lxc.uts.name = jenkins-centos7-x86-64
lxc.arch = x86_64</pre><pre class="literallayout">Also note the container/system naming without underscore in "x86_64" --
the deployed system discards the character when assigning its hostname.
Using "amd64" is another reasonable choice here.</pre></li></ul></div></li><li class="listitem"><p class="simpara">
Add the "name,IP" line for this container to <code class="literal">/etc/lxc/dnsmasq-hosts.conf</code>
  on the host, e.g.:
</p><pre class="screen">jenkins-debian11-mips,10.0.3.245</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>Don’t forget to eventually <code class="literal">systemctl restart lxc-net</code> to apply the
new host reservation!</p></div></li><li class="listitem"><p class="simpara">
Convert a pure LXC container to be managed by LIBVIRT-LXC (and edit config
  markup on the fly — e.g. fix the LXC <code class="literal">dir:/</code> URL schema):
</p><pre class="screen">:; virsh -c lxc:///system domxml-from-native lxc-tools \
    /srv/libvirt/rootfs/jenkins-debian11-armhf/config \
    | sed -e 's,dir:/srv,/srv,' \
    &gt; /tmp/x &amp;&amp; virsh define /tmp/x</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>You may want to tune the default generic 64MB RAM allocation, so your
   launched QEMU containers are not OOM-killed as they exceeded their memory
   cgroup limit. In practice they do not eat that much resident memory, just
   want to have it addressable by VMM, I guess (swap is not very used either),
   at least not until active builds start (then it depends on <code class="literal">make</code> program
   parallelism level you allow, e.g. by <code class="literal">MAXPARMAKES</code> envvar for <code class="literal">ci_build.sh</code>,
   and on the number of Jenkins "executors" assigned to the build agent).</p></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p class="simpara">
It may be needed to revert the generated "os/arch" to <code class="literal">x86_64</code> (and let
   QEMU handle the rest) in the <code class="literal">/tmp/x</code> file, and re-try the definition:
</p><pre class="screen">:; virsh define /tmp/x</pre></li></ul></div></li><li class="listitem"><p class="simpara">
Then <code class="literal">virsh edit jenkins-debian11-armhf</code> (and other containers) to bind-mount
  the common <code class="literal">/home/abuild</code>, adding this tag to their "devices":
</p><pre class="screen">    &lt;filesystem type='mount' accessmode='passthrough'&gt;
      &lt;source dir='/home/abuild'/&gt;
      &lt;target dir='/home/abuild'/&gt;
    &lt;/filesystem&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
Note that generated XML might not conform to current LXC schema so it
   fails validation during save; this can be bypassed with "i" when it asks.
   One such case was however with indeed invalid contents, the "dir:" schema
   removed by example above.
</li></ul></div></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_shepherd_the_herd"></a>Shepherd the herd</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
Monitor deployed container rootfs’es with:
</p><pre class="screen">:; du -ks /srv/libvirt/rootfs/*</pre><p class="simpara">(should have non-trivial size for deployments without fatal infant errors)</p></li><li class="listitem"><p class="simpara">
Mass-edit/review libvirt configurations with:
</p><pre class="screen">:; virsh list --all | awk '{print $2}' \
   | grep jenkins | while read X ; do \
     virsh edit --skip-validate $X ; done</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
…or avoid <code class="literal">--skip-validate</code> when markup is initially good :)
</li></ul></div></li><li class="listitem"><p class="simpara">
Mass-define network interfaces:
</p><pre class="screen">:; virsh list --all | awk '{print $2}' \
   | grep jenkins | while read X ; do \
     virsh dumpxml "$X" | grep "bridge='lxcbr0'" \
     || virsh attach-interface --domain "$X" --config \
        --type bridge --source lxcbr0 ; \
   done</pre></li><li class="listitem"><p class="simpara">
Verify that unique MAC addresses were defined (e.g. <code class="literal">00:16:3e:00:00:01</code>
  tends to pop up often, while <code class="literal">52:54:00:xx:xx:xx</code> are assigned to other
  containers); edit the domain definitions to randomize, if needed:
</p><pre class="screen">:; grep 'mac add' /etc/libvirt/lxc/*.xml | awk '{print $NF" "$1}' | sort</pre></li><li class="listitem"><p class="simpara">
Make sure at least one console device exists (end of file, under the
  network interface definition tags), e.g.:
</p><pre class="screen">    &lt;console type='pty'&gt;
      &lt;target type='lxc' port='0'/&gt;
    &lt;/console&gt;</pre></li><li class="listitem"><p class="simpara">
Populate with <code class="literal">abuild</code> account, as well as with the <code class="literal">bash</code> shell and
  <code class="literal">sudo</code> ability, reporting of assigned IP addresses on the console,
  and SSH server access complete with envvar passing from CI clients
  by virtue of <code class="literal">ssh -o SendEnv='*' container-name</code>:
</p><pre class="screen">:; for ALTROOT in /srv/libvirt/rootfs/*/rootfs/ ; do \
    echo "=== $ALTROOT :" &gt;&amp;2; \
    grep eth0 "$ALTROOT/etc/issue" || ( echo '\S{NAME} \S{VERSION_ID} \n \l@\b ; Current IP(s): \4{eth0} \4{eth1} \4{eth2} \4{eth3}' &gt;&gt; "$ALTROOT/etc/issue" ) ; \
    grep eth0 "$ALTROOT/etc/issue.net" || ( echo '\S{NAME} \S{VERSION_ID} \n \l@\b ; Current IP(s): \4{eth0} \4{eth1} \4{eth2} \4{eth3}' &gt;&gt; "$ALTROOT/etc/issue.net" ) ; \
    groupadd -R "$ALTROOT" -g 399 abuild ; \
    useradd -R "$ALTROOT" -u 399 -g abuild -M -N -s /bin/bash abuild \
    || useradd -R "$ALTROOT" -u 399 -g 399 -M -N -s /bin/bash abuild \
    || { if ! grep -w abuild "$ALTROOT/etc/passwd" ; then \
            echo 'abuild:x:399:399::/home/abuild:/bin/bash' \
            &gt;&gt; "$ALTROOT/etc/passwd" ; \
            echo "USERADDed manually: passwd" &gt;&amp;2 ; \
         fi ; \
         if ! grep -w abuild "$ALTROOT/etc/shadow" ; then \
            echo 'abuild:!:18889:0:99999:7:::' &gt;&gt; "$ALTROOT/etc/shadow" ; \
            echo "USERADDed manually: shadow" &gt;&amp;2 ; \
         fi ; \
       } ; \
    if [ -s "$ALTROOT/etc/ssh/sshd_config" ]; then \
        grep 'AcceptEnv \*' "$ALTROOT/etc/ssh/sshd_config" || ( \
            ( echo ""; echo "# For CI: Allow passing any envvars:"; echo 'AcceptEnv *' ) \
            &gt;&gt; "$ALTROOT/etc/ssh/sshd_config" \
        ) ; \
    fi ; \
   done</pre><p class="simpara">Note that for some reason, in some of those other-arch distros <code class="literal">useradd</code>
fails to find the group anyway; then we have to "manually" add them.</p></li><li class="listitem"><p class="simpara">
Let the host know names/IPs of containers you assigned:
</p><pre class="screen">:; grep -v '#' /etc/lxc/dnsmasq-hosts.conf | while IFS=, read N I ; do \
    getent hosts "$N" &gt;&amp;2 || echo "$I $N" ; \
   done &gt;&gt; /etc/hosts</pre></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_further_setup_of_the_containers"></a>Further setup of the containers</h4></div></div></div><p>See NUT <code class="literal">docs/config-prereqs.txt</code> about dependency package installation
for Debian-based Linux systems.</p><p>It may be wise to not install e.g. documentation generation tools (or at
least not the full set for HTML/PDF generation) in each environment, in
order to conserve space and run-time stress.</p><p>Still, if there are significant version outliers (such as using an older
distribution due to vCPU requirements), it can be installed fully just
to ensure non-regression — that when adapting Makefile rule definitions
to modern tools, we do not lose ability to build with older ones.</p><p>For this, <code class="literal">chroot</code> from the host system can be used, e.g. to improve the
interactive usability for a population of Debian(-compatible) containers
(and to use its networking, while the operating environment in containers
may be not yet configured or still struggling to access the Internet):</p><pre class="screen">:; for ALTROOT in /srv/libvirt/rootfs/*/rootfs/ ; do \
    echo "=== $ALTROOT :" ; \
    chroot "$ALTROOT" apt-get install \
        sudo bash vim mc p7zip p7zip-full pigz pbzip2 git \
   ; done</pre><p>Similarly for <code class="literal">yum</code>-managed systems (CentOS and relatives), though specific
package names can differ, and additional package repositories may need to
be enabled first (see link:config-prereqs.txt for details).</p><p>Note that technically <code class="literal">(sudo) chroot ...</code> can also be used from the CI worker
account on the host system to build in the prepared filesystems without the
overhead of running containers and several copies of Jenkins <code class="literal">agent.jar</code>.</p><p>Also note that set-up of some packages, including the <code class="literal">ca-certificates</code> and
the JDK/JRE, require that the <code class="literal">/proc</code> filesystem is usable in the chroot.
This can be achieved with e.g.:</p><pre class="screen">:; for ALTROOT in /srv/libvirt/rootfs/*/rootfs/ ; do \
    for D in proc ; do \
      echo "=== $ALTROOT/$D :" ; \
      mkdir -p "$ALTROOT/$D" ; \
      mount -o bind,rw "/$D" "$ALTROOT/$D" ; \
    done ; \
   done</pre><p>TODO: Test and document a working NAT and firewall setup for this, to allow
SSH access to the containers via dedicated TCP ports exposed on the host.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_troubleshooting"></a>Troubleshooting</h4></div></div></div><p>Q: Container won’t start, its <code class="literal">virsh console</code> says something like:</p><p>+</p><pre class="screen">Failed to create symlink /sys/fs/cgroup/net_cls: Operation not permitted</pre><p>A: According to <a class="ulink" href="https://bugzilla.redhat.com/show_bug.cgi?id=1770763" target="_top">https://bugzilla.redhat.com/show_bug.cgi?id=1770763</a>
(skip to the end for summary) this can happen when a newer Linux host system
with cgroupsv2 runs an older guest distro that only knows about cgroupsv1,
such as CentOS 7. One workaround is to ensure that the guest systemd does
not try to join host facilities, by setting an explicit empty list:</p><p>+</p><pre class="screen">:; echo 'JoinControllers=' &gt;&gt; "$ALTROOT/etc/systemd/system.conf"</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_connecting_jenkins_to_the_containers"></a>Connecting Jenkins to the containers</h4></div></div></div><p>To cooperate with the jenkins-dynamatrix driving NUT CI builds, each build
environment should be exposed as an individual agent with labels describing
its capabilities.</p><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="_labels"></a>Labels</h5></div></div></div><p>Emulated-CPU container builds are CPU-intensive, so for them we define as
few capabilities as possible: here CI is more interested in checking how
binaries behave on those CPUs, <span class="strong"><strong>not</strong></span> in checking the quality of recipes
(distcheck, Make implementations, etc.), shell scripts or documentation,
which is more efficient to test on native platforms.</p><p>Still, we are interested in results from different compiler suites, so
specify at least one version of each.</p><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>Currently the NUT Jenkinsfile-dynamatrix only looks at various
<code class="literal">COMPILER</code> variants for this use-case, disregarding the versions and
just using one that the environment defaults to.</p></div><p>The reduced set of labels for QEMU workers looks like:</p><pre class="screen">qemu-nut-builder qemu-nut-builder:alldrv
NUT_BUILD_CAPS=drivers:all NUT_BUILD_CAPS=cppunit
OS_FAMILY=linux OS_DISTRO=debian11 GCCVER=10 CLANGVER=11
COMPILER=GCC COMPILER=CLANG
ARCH64=ppc64le ARCH_BITS=64</pre><p>For contrast, a "real" build agent’s set of labels, depending on
presence or known lack of some capabilities, looks like:</p><pre class="screen">doc-builder nut-builder nut-builder:alldrv
NUT_BUILD_CAPS=docs:man NUT_BUILD_CAPS=docs:all
NUT_BUILD_CAPS=drivers:all NUT_BUILD_CAPS=cppunit=no
OS_FAMILY=bsd OS_DISTRO=freebsd12 GCCVER=10 CLANGVER=10
COMPILER=GCC COMPILER=CLANG
ARCH64=amd64 ARCH_BITS=64
SHELL_PROGS=sh SHELL_PROGS=dash SHELL_PROGS=zsh SHELL_PROGS=bash
SHELL_PROGS=csh SHELL_PROGS=tcsh SHELL_PROGS=busybox
MAKE=make MAKE=gmake
PYTHON=python2.7 PYTHON=python3.8</pre></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="_generic_agent_attributes"></a>Generic agent attributes</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
Name: e.g. <code class="literal">ci-debian-altroot--jenkins-debian10-arm64</code> (note the
  pattern for "Conflicts With" detailed below)
</li><li class="listitem">
Remote root directory: preferably unique per agent, to avoid surprises;
  e.g.: <code class="literal">/home/abuild/jenkins-nut-altroots/jenkins-debian10-armel</code>
</li><li class="listitem">
Usage: "Only build jobs with label expressions matching this node"
</li><li class="listitem"><p class="simpara">
Node properties / Environment variables:
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
<code class="literal">PATH+LOCAL</code> ⇒ <code class="literal">/usr/lib/ccache</code>
</li></ul></div></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="_where_to_run_agent_jar"></a>Where to run agent.jar</h5></div></div></div><p>Depending on circumstances of the container, there are several options
available to the NUT CI farm:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
Java can run in the container, efficiently (native CPU, different distro)
  ⇒ the container may be exposed as a standalone host for direct SSH access
  (usually by NAT, exposing SSH on a dedicated port of the host; or by first
  connecting the Jenkins controller with the host as an SSH Build Agent, and
  then calling SSH to the container as a prefix for running the agent), so
  ultimately the build <code class="literal">agent.jar</code> would run in the container.
  Filesystem for the <code class="literal">abuild</code> account may be or not be shared with the host.
</li><li class="listitem">
Java can not run in the container (crashes on emulated CPU, or is too old
  in the agent container’s distro — currently Jenkins requires JRE 8+, but
  eventually will require 11+) ⇒ the agent would run on the host, and then
  the host would <code class="literal">ssh</code> or <code class="literal">chroot</code> (networking not required, but bind-mount
  of <code class="literal">/home/abuild</code> and maybe other paths from host would be needed) called
  for executing <code class="literal">sh</code> steps in the container environment. Either way, home
  directory of the <code class="literal">abuild</code> account is maintained on the host and shared with
  the guest environment, user and group IDs should match.
</li><li class="listitem">
Java is inefficient in the container (operations like un-stashing the source
  succeed but take minutes instead of seconds) ⇒ either of the above
</li></ul></div><p>Methods below involving SSH assume that you have configured a password-less
key authentication from the host machine to the <code class="literal">abuild</code> account in container.
This can be an <code class="literal">ssh-keygen</code> result posted into <code class="literal">authorized_keys</code>, or a trusted
key passed by a chain of ssh agents from a Jenkins credential for connection
to the container-hoster into the container.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
For passing the agent through an SSH connection from host to container,
  so that the <code class="literal">agent.jar</code> runs inside the container environment, configure:
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
Launch method: "Agents via SSH"
</li><li class="listitem">
Host, Credentials, Port: as suitable for accessing the container hoster
</li><li class="listitem"><p class="simpara">
Prefix Start Agent Command: content depends on the container name,
   but generally looks like the example below to report some info about
   the final target platform (and make sure <code class="literal">java</code> is usable) in the
   agent’s log. Note that it ends with un-closed quote and a space char:
</p><pre class="screen">ssh jenkins-debian10-amd64 '( java -version &amp; uname -a ; getconf LONG_BIT; getconf WORD_BIT; wait ) &amp;&amp;</pre></li><li class="listitem">
Suffix Start Agent Command: a single quote to close the text opened above:
</li></ul></div></li></ul></div><pre class="screen">'</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">
The other option is to run the <code class="literal">agent.jar</code> on the host, for all the
  network and filesystem magic the agent does, and only execute shell
  steps in the container. The solution relies on overridden <code class="literal">sh</code> step
  implementation in the jenkins-dynamatrix shared library that uses a
  magic <code class="literal">CI_WRAP_SH</code> environment variable to execute a pipe into the
  container. Such pipes can be <code class="literal">ssh</code> or <code class="literal">chroot</code> with appropriate host
  setup described above.
</p><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>In case of ssh piping, remember that the container’s
<code class="literal">/etc/ssh/sshd_config</code> should <code class="literal">AcceptEnv *</code> and the SSH
server should be restarted after such change.</p></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
Launch method: "Agents via SSH"
</li><li class="listitem">
Host, Credentials, Port: as suitable for accessing the container hoster
</li><li class="listitem"><p class="simpara">
Prefix Start Agent Command: content depends on the container name,
   but generally looks like the example below to report some info about
   the final target platform (and make sure it is accessible) in the
   agent’s log. Note that it ends with a space char, and that the command
   here should not normally print anything into stderr/stdout (this tends
   to confuse the Jenkins Remoting protocol):
</p><pre class="screen">echo PING &gt; /dev/tcp/jenkins-debian11-ppc64el/22 &amp;&amp;</pre></li><li class="listitem">
Suffix Start Agent Command: empty
</li></ul></div></li><li class="listitem"><p class="simpara">
Node properties / Environment variables:
</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">
<code class="literal">CI_WRAP_SH</code> ⇒ `ssh -o SendEnv=<span class="emphasis"><em>*</em></span> "jenkins-debian11-ppc64el" /bin/sh -xe `
</li></ul></div></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="_sequentializing_the_stress"></a>Sequentializing the stress</h5></div></div></div><p>Another aspect of farm management is that emulation is a slow and intensive
operation, so we can not run all agents and execute builds at the same time.</p><p>The current solution relies on <a class="ulink" href="https://github.com/jenkinsci/jenkins/pull/5764" target="_top">https://github.com/jenkinsci/jenkins/pull/5764</a>
to allow build agents to conflict with each other — one picks up a job from
the queue and blocks others from starting; when it is done, another may start.</p><p>Containers can be configured with "Availability ⇒ On demand", with shorter
cycle to switch over faster (the core code sleeps a minute between attempts):</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
In demand delay: <code class="literal">0</code>;
</li><li class="listitem">
Idle delay: <code class="literal">0</code> (Jenkins may change it to <code class="literal">1</code>);
</li><li class="listitem">
Conflicts with: <code class="literal">^ci-debian-altroot--.*$</code> assuming that is the pattern
  for agent definitions in Jenkins — not necessarily linked to hostnames.
</li></ul></div><p>Also, the "executors" count should be reduced to the amount of compilers
in that system (usually 2) and so avoid extra stress of scheduling too many
emulated-CPU builds at once.</p></div></div></div><div xmlns="" class="navfooter nut_footer"><hr />
		Last updated 2022-04-10 14:24:40 -- Network UPS Tools 2.8.0-rc2</div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="apl.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="apl.html">Up</a></td><td width="40%" align="right"> </td></tr><tr><td width="40%" align="left" valign="top"> </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> </td></tr></table></div></body></html>